{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Traget: Included Augmentation to make train data complex\n",
        "- Results: \n",
        "    - Parameters : 7348\n",
        "    - Best Train accuracy : 97.85\n",
        "    - Best Test accuracy : 99.42\n",
        "- Analysis: \n",
        "    - Augmentation makes the training data complex, hence test accuracy is always greater than train accuracy\n",
        "    - The test accuracy has increased from previous 99.40 to 99.42. Consistently seen 99.4 in 2 epochs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating the mean and std dev. of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensor_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "exp = datasets.MNIST('./data', train=True, download=True, transform=tensor_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "exp_train_data = exp.train_data\n",
        "exp_test_data = exp.test_data\n",
        "print(exp_train_data.shape)\n",
        "print(exp_test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Train]\n",
            " - Numpy Shape: (60000, 28, 28)\n",
            " - Tensor Shape: torch.Size([60000, 28, 28])\n",
            " - min: tensor(0.)\n",
            " - max: tensor(1.)\n",
            " - mean: tensor(0.1307)\n",
            " - std: tensor(0.3081)\n",
            " - var: tensor(0.0949)\n"
          ]
        }
      ],
      "source": [
        "exp_train_data = exp.transform(exp_train_data.numpy())\n",
        "print('[Train]')\n",
        "print(' - Numpy Shape:', exp.train_data.cpu().numpy().shape)\n",
        "print(' - Tensor Shape:', exp.train_data.size())\n",
        "print(' - min:', torch.min(exp_train_data))\n",
        "print(' - max:', torch.max(exp_train_data))\n",
        "print(' - mean:', torch.mean(exp_train_data))\n",
        "print(' - std:', torch.std(exp_train_data))\n",
        "print(' - var:', torch.var(exp_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "YtssFUKb-jqx"
      },
      "outputs": [],
      "source": [
        "# Train transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation((-10.0, 10.0), fill=(1,)), # rotate the image within the range of -10 to 10 degrees and fill the empty regions with white color(1) \n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # translate the image up to 10% horizontally and vertically without any rotation(0 degrees)\n",
        "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5), # apply a perspective transformation with distortion up to 10% with probability of 0.5 \n",
        "    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.0)),  # resizes the cropped area to 28x28, 'scale=(0.9, 1.0)' ensures the crop covers 90-100% of the original area.\n",
        "    transforms.ToTensor(),  # Convert to tensor first before erasing \n",
        "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.08), ratio=(0.3, 3.3), value=0.0),  # erase a rectangle in the image with realtive size of 'scale' with 0.3 to 3.3 ratio, the recatangle is black in color(0). Probability of erasing is 0.25 \n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize it with mean and std dev of train_data. \n",
        "])\n",
        "\n",
        "# Test transformations\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # use the train data's mean and std dev\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4A84rlfDA23",
        "outputId": "2cc0a4ce-205a-4bdf-ffb7-753398ed571f"
      },
      "outputs": [],
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) # downloading the train data and applying train transforms\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) # downloading the test data and applying test transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8OLDR79DrHG",
        "outputId": "66a9b95a-92e4-4194-c6a4-878b0ac5942a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available? False\n"
          ]
        }
      ],
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h"
      },
      "source": [
        "# The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "7FXQlB9kH1ov"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         dropout_value = 0.05\n",
        "        \n",
        "#         # Input Block - RF: 3\n",
        "#         self.convblock1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 8, 3, padding=1, bias=False),  # RF: 3\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         # Conv Block 1 - RF: 5\n",
        "#         self.convblock2 = nn.Sequential(\n",
        "#             nn.Conv2d(8, 8, 3, padding=1, bias=False),  # RF: 5\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         # Transition Block 1 - RF: 6\n",
        "#         self.pool1 = nn.MaxPool2d(2, 2)  # RF: 6\n",
        "#         self.convblock3 = nn.Sequential(\n",
        "#             nn.Conv2d(8, 12, 1, bias=False),  # RF: 6\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         # Conv Block 2 - RF: 14\n",
        "#         self.convblock4 = nn.Sequential(\n",
        "#             nn.Conv2d(12, 12, 3, padding=1, bias=False),  # RF: 10\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value),\n",
        "#             nn.Conv2d(12, 12, 3, padding=1, bias=False),  # RF: 14\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         # Transition Block 2 - RF: 16\n",
        "#         self.pool2 = nn.MaxPool2d(2, 2)  # RF: 16\n",
        "#         self.convblock5 = nn.Sequential(\n",
        "#             nn.Conv2d(12, 12, 1, bias=False),  # RF: 16\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         # Conv Block 3 - RF: 28\n",
        "#         self.convblock6 = nn.Sequential(\n",
        "#             nn.Conv2d(12, 12, 3, padding=1, bias=False),  # RF: 20\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value),\n",
        "#             nn.Conv2d(12, 12, 3, padding=1, bias=False),  # RF: 24\n",
        "#             nn.BatchNorm2d(12),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value),\n",
        "#             nn.Conv2d(12, 10, 3, padding=1, bias=False),  # RF: 28\n",
        "#             nn.BatchNorm2d(10),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         )\n",
        "\n",
        "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.convblock1(x)\n",
        "#         x = self.convblock2(x)\n",
        "#         x = self.pool1(x)\n",
        "#         x = self.convblock3(x)\n",
        "#         x = self.convblock4(x)\n",
        "#         x = self.pool2(x)\n",
        "#         x = self.convblock5(x)\n",
        "#         x = self.convblock6(x)\n",
        "#         x = self.gap(x)\n",
        "#         x = x.view(-1, 10)\n",
        "#         return F.log_softmax(x, dim=-1)\n",
        "    \n",
        "\n",
        "       \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo"
      },
      "source": [
        "# Model Params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5skB97zIJQQe",
        "outputId": "b1fe5230-279d-40c4-a500-49eb192cd239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\users\\chaitra.saik\\appdata\\local\\miniconda3\\lib\\site-packages (1.5.1)\n",
            "cpu\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              72\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "              ReLU-3            [-1, 8, 28, 28]               0\n",
            "           Dropout-4            [-1, 8, 28, 28]               0\n",
            "            Conv2d-5            [-1, 8, 28, 28]             576\n",
            "       BatchNorm2d-6            [-1, 8, 28, 28]              16\n",
            "              ReLU-7            [-1, 8, 28, 28]               0\n",
            "           Dropout-8            [-1, 8, 28, 28]               0\n",
            "         MaxPool2d-9            [-1, 8, 14, 14]               0\n",
            "           Conv2d-10           [-1, 12, 14, 14]              96\n",
            "      BatchNorm2d-11           [-1, 12, 14, 14]              24\n",
            "             ReLU-12           [-1, 12, 14, 14]               0\n",
            "          Dropout-13           [-1, 12, 14, 14]               0\n",
            "           Conv2d-14           [-1, 12, 14, 14]           1,296\n",
            "      BatchNorm2d-15           [-1, 12, 14, 14]              24\n",
            "             ReLU-16           [-1, 12, 14, 14]               0\n",
            "          Dropout-17           [-1, 12, 14, 14]               0\n",
            "           Conv2d-18           [-1, 12, 14, 14]           1,296\n",
            "      BatchNorm2d-19           [-1, 12, 14, 14]              24\n",
            "             ReLU-20           [-1, 12, 14, 14]               0\n",
            "          Dropout-21           [-1, 12, 14, 14]               0\n",
            "        MaxPool2d-22             [-1, 12, 7, 7]               0\n",
            "           Conv2d-23             [-1, 12, 7, 7]             144\n",
            "      BatchNorm2d-24             [-1, 12, 7, 7]              24\n",
            "             ReLU-25             [-1, 12, 7, 7]               0\n",
            "          Dropout-26             [-1, 12, 7, 7]               0\n",
            "           Conv2d-27             [-1, 12, 7, 7]           1,296\n",
            "      BatchNorm2d-28             [-1, 12, 7, 7]              24\n",
            "             ReLU-29             [-1, 12, 7, 7]               0\n",
            "          Dropout-30             [-1, 12, 7, 7]               0\n",
            "           Conv2d-31             [-1, 12, 7, 7]           1,296\n",
            "      BatchNorm2d-32             [-1, 12, 7, 7]              24\n",
            "             ReLU-33             [-1, 12, 7, 7]               0\n",
            "          Dropout-34             [-1, 12, 7, 7]               0\n",
            "           Conv2d-35             [-1, 10, 7, 7]           1,080\n",
            "      BatchNorm2d-36             [-1, 10, 7, 7]              20\n",
            "             ReLU-37             [-1, 10, 7, 7]               0\n",
            "          Dropout-38             [-1, 10, 7, 7]               0\n",
            "AdaptiveAvgPool2d-39             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 7,348\n",
            "Trainable params: 7,348\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.68\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.71\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Skipping C:\\Users\\chaitra.saik\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch-2.2.0.dist-info due to invalid metadata entry 'name'\n",
            "WARNING: Skipping C:\\Users\\chaitra.saik\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch-2.2.0.dist-info due to invalid metadata entry 'name'\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "# model = Net().to(device)\n",
        "model = models.Model_4_augmnetation().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = Net().to(device)\n",
        "model = models.Model_4_augmnetation().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4) # defining the optimizer with leraning rate of 0.01\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "#     optimizer,\n",
        "#     T_0=4,  # Initial restart interval\n",
        "#     T_mult=1,  # Multiplier for restart interval\n",
        "#     eta_min=1e-6  # Minimum learning rate\n",
        "# )\n",
        "\n",
        "# Using the OneCycleLR scheduler for dynamic learning rate adjustment.\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.01,  # Maximum learning rate during the cycle.\n",
        "    epochs=15,  # Total number of epochs for training.\n",
        "    steps_per_epoch=len(train_loader),  # Number of steps in one epoch (based on train loader size).\n",
        "    pct_start=0.3,  # Percentage of the cycle for increasing the learning rate.\n",
        "    div_factor=10,  # Factor by which the initial learning rate is divided from max_lr.\n",
        "    final_div_factor=100,  # Factor by which the learning rate is reduced at the end of the cycle.\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "fbkF2nN_LYIb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5Le6FYHhc8",
        "outputId": "a0178258-31c3-4644-b93f-a93181ad1760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.3920605480670929 Batch_id=937 Accuracy=80.65: 100%|██████████| 938/938 [01:12<00:00, 12.86it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1854, Accuracy: 9667/10000 (96.67%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.10264067351818085 Batch_id=937 Accuracy=93.45: 100%|██████████| 938/938 [01:06<00:00, 14.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1372, Accuracy: 9684/10000 (96.84%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.22327573597431183 Batch_id=937 Accuracy=94.28: 100%|██████████| 938/938 [01:04<00:00, 14.46it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0608, Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.2098611444234848 Batch_id=937 Accuracy=95.04: 100%|██████████| 938/938 [01:04<00:00, 14.65it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0559, Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0844479352235794 Batch_id=937 Accuracy=95.54: 100%|██████████| 938/938 [01:06<00:00, 14.20it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9846/10000 (98.46%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.062942735850811 Batch_id=937 Accuracy=96.09: 100%|██████████| 938/938 [01:05<00:00, 14.24it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0473, Accuracy: 9854/10000 (98.54%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.05413801968097687 Batch_id=937 Accuracy=96.31: 100%|██████████| 938/938 [01:05<00:00, 14.41it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9891/10000 (98.91%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.051060888916254044 Batch_id=937 Accuracy=96.54: 100%|██████████| 938/938 [01:04<00:00, 14.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.3372788727283478 Batch_id=937 Accuracy=96.79: 100%|██████████| 938/938 [01:06<00:00, 14.04it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.008700123988091946 Batch_id=937 Accuracy=97.10: 100%|██████████| 938/938 [01:04<00:00, 14.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.06875372678041458 Batch_id=937 Accuracy=97.35: 100%|██████████| 938/938 [01:05<00:00, 14.32it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9921/10000 (99.21%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.055069927126169205 Batch_id=937 Accuracy=97.56: 100%|██████████| 938/938 [01:02<00:00, 15.01it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0229, Accuracy: 9928/10000 (99.28%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.03588985651731491 Batch_id=937 Accuracy=97.61: 100%|██████████| 938/938 [01:05<00:00, 14.23it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0204, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.031018974259495735 Batch_id=937 Accuracy=97.78: 100%|██████████| 938/938 [01:05<00:00, 14.30it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.01290989015251398 Batch_id=937 Accuracy=97.85: 100%|██████████| 938/938 [01:06<00:00, 14.19it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 9941/10000 (99.41%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    \n",
        "    test(model, device, test_loader)\n",
        "    # scheduler.step() # If in case of Cosine Annealing Warm, or based on other scheduler, scheduler.step.() has to uncommented. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changed wdecay to e-5\n",
        "# changed erase prob to 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
